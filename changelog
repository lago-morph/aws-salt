=============================================================
2024-11-26 Packer tutorials

Learning Packer using the Hashicorp AWS
[tutorial](https://developer.hashicorp.com/packer/tutorials/aws-get-started/aws-get-started-build-image).

After building, was able to look at the image with `aws ec2 describe-images
--owners self`.

When I tried to build two instances in parallel packer errored out with
something about a VPC not being available.  Turned out this was because I was
trying to build one of them in a region where I didn't have a default VPC.


=============================================================
2024-11-26-2 Design of "better" system for bootstrapping salt

What I want to do here is come up with a way to leverage packer, terraform,
and salt to built infrastructure on AWS in a professional and robust way.

- Use packer to build AMIs that have the software I need preinstalled
- Use tags on instances to determine the role of an instance for configuration
using salt.pillar.ec2_pillar
https://docs.saltproject.io/en/latest/ref/pillar/all/salt.pillar.ec2_pillar.html
- Store the salt configuration files in a private github repository, and pull
them using proper AWS secrets

What I need is:
A base image with salt-minion installed and the ec2_pillar
A salt server image that adds gh and salt-master, and has a startup script to
get credentials from the secrets store and clone the repo and set up symbolic
links

Salt master, also the bastion host.

Ok, anyway, I got packer creating a Ubuntu 20.04 image with salt-minion
installed, and a second one with salt-master, salt-minion, and gh installed.
Next I need to figure out how to use the AWS secret store and pre-install a
github access token on the salt-master.  It should be parameterized by
cluster.

So, the data structure I need in sls:
<cluster>/salt-repo/host/<hostname>
<cluster>/salt-repo/organization/<orgname>
<cluster>/salt-repo/repository/<reponame>
<cluster>/salt-repo/access-token/<token>
<cluster>/host-group/<groupname>/num-instances/<num>
<cluster>/host-group/<groupname>/role/<rolename1>
<cluster>/host-group/<groupname>/role/<rolename2> (etc.)
Each instance will have tags:
cluster = <cluster>
role:<rolename1>
role:<rolename2>

Available roles:
salt-master
salt-minion
external-ssh
webserver
load-balancer
monitor

Use internal DNS server to serve the internal IP address of the salt-master.

There is a way to get instance metadata from a salt grain.
https://docs.saltproject.io/en/latest/ref/grains/all/salt.grains.metadata.html

We can get pillar data from an arbitrary command and parse it as JSON.
https://docs.saltproject.io/en/latest/ref/pillar/all/salt.pillar.cmd_json.html

This would be useful for templating the salt://top.sls file.
Salt has the concept of a master top, a plugin based system for generating the
top.sls file.
https://docs.saltproject.io/en/3006/topics/master_tops/index.html#master-tops-system

It seems like this is necessary to make nodes and roles generic.
https://docs.saltproject.io/en/3006/topics/master_tops/index.html#master-tops-system
One thing is that it seems to prefer class-based node classifiers.
I guess I could query AWS and rewrite the inventory periodically...
and use the saltclass node classifier.

So the way to clone git repos without leaving auth info on the server:
1. Get a github access token from a secrets manager
2. run echo "<github access token>" | gh auth login --with-token
3. use gh to clone the repository, e.g., gh repo clone lago-morph/aws-salt
4. Then log out with gh auth logout.  This deletes the token from the
filesystem.  It would be easier if the gh repo clone command took an auth
token as an argument.  I'm sure there's a better way to do this.

=============================================================
2024-11-27 More thoughts on design

cluster tools (aws-salt)
-------------
GitHub repo with the terraform module and packer stuff defined

`config` target in Makefile to create backend.tf file that points to the S3
bucket and DynamoDB table with state based on a cluster name.

Separate target in separate directory to create initial state and set key
/cluster/<cluster-name> = <type-name>

Separate targets to set parameters for cluster repo, cluster type and cluster based on input JSON file

Note, salt master in a cluster should have IAM role to access cluster type,
cluster repo and cluster keys for its cluster and nothing else

cluster AMIs
------------

AMI for the salt master should be tagged type = salt-master
AMI for all other hosts should be tagged type = host

cluster type repo
-----------------
A github repo that has salt state files and saltclass definition for hosts in
the cluster.  Defined in AWS Systems Manager Parameter Store (smps)

/cluster-repo/<type-repo>/organization
/cluster-repo/<type-repo>/repository
/cluster-repo/<type-repo>/key-id
/cluster-repo/<type-repo>/secret-key

cluster type
------------
Combination of a GitHub repo and branch name.  Allows use of for instance dev,
staging, and prod branches represented by different clusters.
Defined in smps.

/cluster-type/<type-name>/type-repo = <type-repo>
/cluster-type/<type-name>/branch = <branch>

cluster instance
----------------
Specifies a cluster type and a name for the cluster that is unique
to the account and region in which the cluster will be deployed.  Defined in
smps.

This is a JSON structure to make it easier to process in terraform.  The
other keys are lookup only, whereas with the cluster I have to iterate over
the host classes.

/cluster/<cluster-name> = 
{ 
   "cluster-type" : <type-name>, 
   "hostclass": [
     { "class-name" : <classname>,
	   "num-hosts" : <num hosts>,
	   "public-ip" : <true/false>
	 }
   ]   
}
The instance will be tagged with the cluster name and type.

Need to manually ensure that the public attribute on the host class
corresponds with how the host is defined in the saltclass and salt state
files.


Stuff to build:
- scripts to manipulate cluster repo, cluster type, and cluster
  - for now this can be template that is modified with AWS CLI commands
- script to build AMIs for hosts and salt master, then set the smps keys /ami/hosts and /ami/salt-master
- module to provision cluster based on cluster keys
  - will explicitly build just one salt master and set up DNS for it
  - takes a 
- Simple cluster definition (salt states and saltclass)
- salt-master firstboot script to grab repo and set symbolic links


Also, figured out how to do git with ssh keys.
Add a deploy key as the public key of the ssh key to the repository.
Clone the repository using the following syntax:
git clone git@github.com:lago-morph/aws-salt-simple.git

And it uses the ssh key for authentication.

Should make the key used by the install process read-only, and individual
users when they log in can forward their keys and be able to do something
better.

Generate an SSH key to use for read-only access to the aws-salt-simple
repository

ssh-keygen -t ed25519 -C "Read-only Key for aws-salt-simple" -f aws-salt-simple -N ""

What I want in the future is a CLI interface for editing the cluster repo,
type, and clusters themselves.  Like the kubectl cli.
Or alternatively an interactive one, where you can choose from valid values
based on what is already in ssm.


Inputs to the terraform module:
admin-users = [
  testuser: {
    public_keys = [
	  "<publickeys of whoever can login to this user>"
	]
	sudo = true
  }
] (default [])
cluster-name = "simple-cluster-name"
region =  (default "us-east-1")
vpc_cidr = (default "10.0.0.0/16")

I've been struggling with Terraform for hours, trying to get it to use my
parameterization.  But I think I realized that there is another way:
- define the cluster repo and cluster type in ssm
- define the cluster name and the number of hosts in each hostclass as a json
file that gets fed to terraform

The json file could even be stored in ssm, and pulled before the terraform
run.

The other way to do this is to do a terraform apply with the -target option to
pull the ssm data into the state file.  Then I can use it locally in for_each
or count loops to define the resources.

I also realize I need to basically create a single list which contains a value
and a sequence number for each host in all hostgroups.  Terraform does poorly
with nested lists unless they are associated to some other resource.

I am going to work around this for now by putting the json that defines the
cluster as an input to the terraform configuration.

I just spent 2 hours trying to debug an AWS IAM issue.  And it looks like it
was because terraform doesn't know that you have to restart an instance when
you change its instance profile (which also changes its role).  Or it might
have been the old version of the AWS CLI.  Jury is still out.

Ok, I haven't written anything in here for a while.  Here's where I stand:
- I create hosts based on a JSON document defining the cluster
- They boot up using AMIs I created with Packer to pre-install stuff.
- I'm working on getting security permissions so that I can access the ssm
parameters.

I haven't been good about checking in git changes in small amounts.  Taking
care of that now.

Well, unfortunately it wasn't not restarting the instance.  It still doesn't
work.  But creating the role through the AWS console works fine.

First, I will whittle the permissions down in the manually-created role to
make sure it isn't something there.  Then I'll delve into maybe the instance
profile.

Ok, so it looks like it takes a while for the security policies on a role to
update on an instance.  Because I removed something, that made it stop
working, then I put it back, and it continued to not work.

So I'm going to use a managed policy,
arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
and see if that helps.

That did.  I don't know what was wrong, but at this point I'm not going to
spend any more time on it.

Feeling my way around using an ssh key to get the repo.

aws ssm get-parameter --name "/cluster_repo/simple_type/private_key" | jq .Parameter.Value > ~/.ssh/aws-salt-simple
aws ssm get-parameter --name "/cluster_repo/simple_type/public_key" | jq .Parameter.Value > ~/.ssh/aws-salt-simple.pub

I got this kind of working.  I get things in a strange format coming in from
the keystore.  This worked except I now need some line breaks:

aws ssm get-parameter --with-decryption --name
"/cluster_repo/simple_type/private_key" | jq .Parameter.Value | sed
"s/\\\n//g" | sed "s/\"//g" > .ssh/aws-salt-simple

chmod 600 .ssh/aws-salt-simple
ssh-agent
(edit file)
ssh-add .ssh/aws-salt-simple

The following clones the repo (the first one is to get the host key)

ssh -o "StrictHostKeyChecking no" git@github.com
git clone git@github.com:lago-morph/aws-salt-simple.git

Then these same things from the other project git install script
sudo mv aws-salt-simple /opt
sudo ln -s /opt/aws-salt-simple/salt /srv/salt
sudo ln -s /opt/aws-salt-simple/pillar /srv/pillar
sudo ln -s /opt/aws-salt-simple/salt/master.d/* /etc/salt/master.d/

Ok, in order to get access to this stuff, I need to query my own tags to get
cluster_type, then go down the line.


Side note - found a great way to set aws region from metadata here
https://gist.github.com/quiver/87f93bc7df6da7049d41

This can go in UserData:
#!/bin/bash
yum install -y jq
REGION=`curl -s
http://169.254.169.254/latest/dynamic/instance-identity/document | jq .region
-r`
sudo -u ec2-user aws configure set region $REGION

This can be executed on the command line:
aws configure set region `curl --silent http://169.254.169.254/latest/dynamic/instance-identity/document | jq -r .region`
Of course, by default the instance tags are not exposed as part of the
metadata at http://169.254.169.254/latest/meta-data/

Ok, here we are:

export CLUSTER_TYPE=$(curl http://169.254.169.254/latest/meta-data/tags/instance/cluster_type)
aws configure set region `curl --silent http://169.254.169.254/latest/dynamic/instance-identity/document | jq -r .region`
export TYPE_REPO=$(aws ssm get-parameter --name "/cluster_type/$CLUSTER_TYPE/type_repo" | jq -r .Parameter.Value)
export BRANCH=$(aws ssm get-parameter --name "/cluster_type/$CLUSTER_TYPE/branch" | jq -r .Parameter.Value)
export REPOSITORY=$(aws ssm get-parameter --name "/cluster_repo/$TYPE_REPO/repository" | jq -r .Parameter.Value)
export ORGANIZATION=$(aws ssm get-parameter --name "/cluster_repo/$TYPE_REPO/organization" | jq -r .Parameter.Value)
export PRIVATE_KEY=$(aws ssm get-parameter --with-decryption --name "/cluster_repo/$TYPE_REPO/private_key" | jq -r .Parameter.Value)
export PUBLIC_KEY=$(aws ssm get-parameter --name "/cluster_repo/$TYPE_REPO/public_key" | jq -r .Parameter.Value)

echo $PRIVATE_KEY | sed "s/- /-\n/" | sed "s/ -/\n-/" > .ssh/repokey
chmod 600 .ssh/repokey
ssh-agent
ssh-add .ssh/repokey

GIT_SSH_COMMAND="ssh -o StrictHostKeyChecking=accept-new" git clone
git@github.com:$ORGANIZATION/$REPOSITORY.git

Then these same things from the other project git install script
sudo mv $REPOSITORY /opt
cd /opt/$REPOSITORY
git checkout $BRANCH
sudo ln -s /opt/$REPOSITORY/salt /srv/salt 
sudo ln -s /opt/$REPOSITORY/pillar /srv/pillar 
sudo ln -s /opt/$REPOSITORY/salt/master.d/* /etc/salt/master.d/

sudo systemctl restart salt-master

It is late, I will turn this into a UserData thing tomorrow.
