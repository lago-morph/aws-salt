===============================================================================
2024-11-26 Packer tutorials

Learning Packer using the Hashicorp AWS
[tutorial](https://developer.hashicorp.com/packer/tutorials/aws-get-started/aws-get-started-build-image).

After building, was able to look at the image with `aws ec2 describe-images
--owners self`.

When I tried to build two instances in parallel packer errored out with
something about a VPC not being available.  Turned out this was because I was
trying to build one of them in a region where I didn't have a default VPC.


===============================================================================
2024-11-26-2 Design of "better" system for bootstrapping salt

What I want to do here is come up with a way to leverage packer, terraform,
and salt to built infrastructure on AWS in a professional and robust way.

- Use packer to build AMIs that have the software I need preinstalled
- Use tags on instances to determine the role of an instance for configuration
using salt.pillar.ec2_pillar
https://docs.saltproject.io/en/latest/ref/pillar/all/salt.pillar.ec2_pillar.html
- Store the salt configuration files in a private github repository, and pull
them using proper AWS secrets

What I need is:
A base image with salt-minion installed and the ec2_pillar
A salt server image that adds gh and salt-master, and has a startup script to
get credentials from the secrets store and clone the repo and set up symbolic
links

Salt master, also the bastion host.

Ok, anyway, I got packer creating a Ubuntu 20.04 image with salt-minion
installed, and a second one with salt-master, salt-minion, and gh installed.
Next I need to figure out how to use the AWS secret store and pre-install a
github access token on the salt-master.  It should be parameterized by
cluster.

So, the data structure I need in sls:
<cluster>/salt-repo/host/<hostname>
<cluster>/salt-repo/organization/<orgname>
<cluster>/salt-repo/repository/<reponame>
<cluster>/salt-repo/access-token/<token>
<cluster>/host-group/<groupname>/num-instances/<num>
<cluster>/host-group/<groupname>/role/<rolename1>
<cluster>/host-group/<groupname>/role/<rolename2> (etc.)
Each instance will have tags:
cluster = <cluster>
role:<rolename1>
role:<rolename2>

Available roles:
salt-master
salt-minion
external-ssh
webserver
load-balancer
monitor

Use internal DNS server to serve the internal IP address of the salt-master.

There is a way to get instance metadata from a salt grain.
https://docs.saltproject.io/en/latest/ref/grains/all/salt.grains.metadata.html

We can get pillar data from an arbitrary command and parse it as JSON.
https://docs.saltproject.io/en/latest/ref/pillar/all/salt.pillar.cmd_json.html

This would be useful for templating the salt://top.sls file.
Salt has the concept of a master top, a plugin based system for generating the
top.sls file.
https://docs.saltproject.io/en/3006/topics/master_tops/index.html#master-tops-system

It seems like this is necessary to make nodes and roles generic.
https://docs.saltproject.io/en/3006/topics/master_tops/index.html#master-tops-system
One thing is that it seems to prefer class-based node classifiers.
I guess I could query AWS and rewrite the inventory periodically...
and use the saltclass node classifier.

So the way to clone git repos without leaving auth info on the server:
1. Get a github access token from a secrets manager
2. run echo "<github access token>" | gh auth login --with-token
3. use gh to clone the repository, e.g., gh repo clone lago-morph/aws-salt
4. Then log out with gh auth logout.  This deletes the token from the
filesystem.  It would be easier if the gh repo clone command took an auth
token as an argument.  I'm sure there's a better way to do this.

===============================================================================
2024-11-27 More thoughts on design

cluster tools (aws-salt)
-------------
GitHub repo with the terraform module and packer stuff defined

`config` target in Makefile to create backend.tf file that points to the S3
bucket and DynamoDB table with state based on a cluster name.

Separate target in separate directory to create initial state and set key
/cluster/<cluster-name> = <type-name>

Separate targets to set parameters for cluster repo, cluster type and cluster based on input JSON file

Note, salt master in a cluster should have IAM role to access cluster type,
cluster repo and cluster keys for its cluster and nothing else

cluster AMIs
------------

AMI for the salt master should be tagged type = salt-master
AMI for all other hosts should be tagged type = host

cluster type repo
-----------------
A github repo that has salt state files and saltclass definition for hosts in
the cluster.  Defined in AWS Systems Manager Parameter Store (smps)

/cluster-repo/<type-repo>/organization
/cluster-repo/<type-repo>/repository
/cluster-repo/<type-repo>/key-id
/cluster-repo/<type-repo>/secret-key

cluster type
------------
Combination of a GitHub repo and branch name.  Allows use of for instance dev,
staging, and prod branches represented by different clusters.
Defined in smps.

/cluster-type/<type-name>/type-repo = <type-repo>
/cluster-type/<type-name>/branch = <branch>

cluster instance
----------------
Specifies a cluster type and a name for the cluster that is unique
to the account and region in which the cluster will be deployed.  Defined in
smps.

This is a JSON structure to make it easier to process in terraform.  The
other keys are lookup only, whereas with the cluster I have to iterate over
the host classes.

/cluster/<cluster-name> = 
{ 
   "cluster-type" : <type-name>, 
   "hostclass": [
     { "class-name" : <classname>,
	   "num-hosts" : <num hosts>,
	   "public-ip" : <true/false>
	 }
   ]   
}
The instance will be tagged with the cluster name and type.

Need to manually ensure that the public attribute on the host class
corresponds with how the host is defined in the saltclass and salt state
files.


Stuff to build:
- scripts to manipulate cluster repo, cluster type, and cluster
  - for now this can be template that is modified with AWS CLI commands
- script to build AMIs for hosts and salt master, then set the smps keys /ami/hosts and /ami/salt-master
- module to provision cluster based on cluster keys
  - will explicitly build just one salt master and set up DNS for it
  - takes a 
- Simple cluster definition (salt states and saltclass)
- salt-master firstboot script to grab repo and set symbolic links


Also, figured out how to do git with ssh keys.
Add a deploy key as the public key of the ssh key to the repository.
Clone the repository using the following syntax:
git clone git@github.com:lago-morph/aws-salt-simple.git

And it uses the ssh key for authentication.

Should make the key used by the install process read-only, and individual
users when they log in can forward their keys and be able to do something
better.

Generate an SSH key to use for read-only access to the aws-salt-simple
repository

ssh-keygen -t ed25519 -C "Read-only Key for aws-salt-simple" -f aws-salt-simple -N ""

What I want in the future is a CLI interface for editing the cluster repo,
type, and clusters themselves.  Like the kubectl cli.
Or alternatively an interactive one, where you can choose from valid values
based on what is already in ssm.


Inputs to the terraform module:
admin-users = [
  testuser: {
    public_keys = [
	  "<publickeys of whoever can login to this user>"
	]
	sudo = true
  }
] (default [])
cluster-name = "simple-cluster-name"
region =  (default "us-east-1")
vpc_cidr = (default "10.0.0.0/16")

I've been struggling with Terraform for hours, trying to get it to use my
parameterization.  But I think I realized that there is another way:
- define the cluster repo and cluster type in ssm
- define the cluster name and the number of hosts in each hostclass as a json
file that gets fed to terraform

The json file could even be stored in ssm, and pulled before the terraform
run.

The other way to do this is to do a terraform apply with the -target option to
pull the ssm data into the state file.  Then I can use it locally in for_each
or count loops to define the resources.

I also realize I need to basically create a single list which contains a value
and a sequence number for each host in all hostgroups.  Terraform does poorly
with nested lists unless they are associated to some other resource.

I am going to work around this for now by putting the json that defines the
cluster as an input to the terraform configuration.

I just spent 2 hours trying to debug an AWS IAM issue.  And it looks like it
was because terraform doesn't know that you have to restart an instance when
you change its instance profile (which also changes its role).  Or it might
have been the old version of the AWS CLI.  Jury is still out.

Ok, I haven't written anything in here for a while.  Here's where I stand:
- I create hosts based on a JSON document defining the cluster
- They boot up using AMIs I created with Packer to pre-install stuff.
- I'm working on getting security permissions so that I can access the ssm
parameters.

I haven't been good about checking in git changes in small amounts.  Taking
care of that now.

Well, unfortunately it wasn't not restarting the instance.  It still doesn't
work.  But creating the role through the AWS console works fine.

First, I will whittle the permissions down in the manually-created role to
make sure it isn't something there.  Then I'll delve into maybe the instance
profile.

Ok, so it looks like it takes a while for the security policies on a role to
update on an instance.  Because I removed something, that made it stop
working, then I put it back, and it continued to not work.

So I'm going to use a managed policy,
arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
and see if that helps.

That did.  I don't know what was wrong, but at this point I'm not going to
spend any more time on it.

Feeling my way around using an ssh key to get the repo.

aws ssm get-parameter --name "/cluster_repo/simple_type/private_key" | jq .Parameter.Value > ~/.ssh/aws-salt-simple
aws ssm get-parameter --name "/cluster_repo/simple_type/public_key" | jq .Parameter.Value > ~/.ssh/aws-salt-simple.pub

I got this kind of working.  I get things in a strange format coming in from
the keystore.  This worked except I now need some line breaks:

aws ssm get-parameter --with-decryption --name
"/cluster_repo/simple_type/private_key" | jq .Parameter.Value | sed
"s/\\\n//g" | sed "s/\"//g" > .ssh/aws-salt-simple

chmod 600 .ssh/aws-salt-simple
ssh-agent
(edit file)
ssh-add .ssh/aws-salt-simple

The following clones the repo (the first one is to get the host key)

ssh -o "StrictHostKeyChecking no" git@github.com
git clone git@github.com:lago-morph/aws-salt-simple.git

Then these same things from the other project git install script
sudo mv aws-salt-simple /opt
sudo ln -s /opt/aws-salt-simple/salt /srv/salt
sudo ln -s /opt/aws-salt-simple/pillar /srv/pillar
sudo ln -s /opt/aws-salt-simple/salt/master.d/* /etc/salt/master.d/

Ok, in order to get access to this stuff, I need to query my own tags to get
cluster_type, then go down the line.


Side note - found a great way to set aws region from metadata here
https://gist.github.com/quiver/87f93bc7df6da7049d41

This can go in UserData:
#!/bin/bash
yum install -y jq
REGION=`curl -s
http://169.254.169.254/latest/dynamic/instance-identity/document | jq .region
-r`
sudo -u ec2-user aws configure set region $REGION

This can be executed on the command line:
aws configure set region `curl --silent http://169.254.169.254/latest/dynamic/instance-identity/document | jq -r .region`
Of course, by default the instance tags are not exposed as part of the
metadata at http://169.254.169.254/latest/meta-data/

Ok, here we are:

export CLUSTER_TYPE=$(curl http://169.254.169.254/latest/meta-data/tags/instance/cluster_type)
aws configure set region `curl --silent http://169.254.169.254/latest/dynamic/instance-identity/document | jq -r .region`
export TYPE_REPO=$(aws ssm get-parameter --name "/cluster_type/$CLUSTER_TYPE/type_repo" | jq -r .Parameter.Value)
export BRANCH=$(aws ssm get-parameter --name "/cluster_type/$CLUSTER_TYPE/branch" | jq -r .Parameter.Value)
export REPOSITORY=$(aws ssm get-parameter --name "/cluster_repo/$TYPE_REPO/repository" | jq -r .Parameter.Value)
export ORGANIZATION=$(aws ssm get-parameter --name "/cluster_repo/$TYPE_REPO/organization" | jq -r .Parameter.Value)
export PRIVATE_KEY=$(aws ssm get-parameter --with-decryption --name "/cluster_repo/$TYPE_REPO/private_key" | jq -r .Parameter.Value)
export PUBLIC_KEY=$(aws ssm get-parameter --name "/cluster_repo/$TYPE_REPO/public_key" | jq -r .Parameter.Value)

echo $PRIVATE_KEY | sed "s/- /-\n/" | sed "s/ -/\n-/" > .ssh/repokey
chmod 600 .ssh/repokey
ssh-agent
ssh-add .ssh/repokey

GIT_SSH_COMMAND="ssh -o StrictHostKeyChecking=accept-new" git clone
git@github.com:$ORGANIZATION/$REPOSITORY.git

Then these same things from the other project git install script
sudo mv $REPOSITORY /opt
cd /opt/$REPOSITORY
git checkout $BRANCH
sudo ln -s /opt/$REPOSITORY/salt /srv/salt 
sudo ln -s /opt/$REPOSITORY/pillar /srv/pillar 
sudo ln -s /opt/$REPOSITORY/salt/master.d/* /etc/salt/master.d/

sudo systemctl restart salt-master

It is late, I will turn this into a UserData thing tomorrow.

===============================================================================
2024-11-29 Data model

NOTE - THIS CHANGES TERMS USED ABOVE IN THIS CHANGELOG

This is a "GitOps" driven cluster system.

There is one repo, aws-salt, that contains the terraform module and helper
scripts (e.g., packer build).

There is an ultimate upstream repository cluster-type-template from which 
cluster-type repositories should fork.

For each cluster-type, there is one upstream repository forked from
repo cluster-template, which targets the
region you are doing development in for that type of cluster.  For additional
regions, you will fork the cluster-type repo.

aws-salt

cluster-type-template
   ||
 (fork)
   \/
cluster-redis-default == (fork) ==> cluster-type-redis-us-east-2
   ||
(branch)
   \/
cluster-redis branch-n

So a "cluster" is a cluster-type repository (potentially forked with the
region redefined), plus a branch of that repository.  There cannot be multiple
clusters with different names with the same repo and branch.  Just create a
new branch if you want another cluster with an identical configuration.

The cluster name is (cluster-type)-(branch-name), and must be unique by region
and account.

For convenience we will store the terraform state in with each cluster-repo
and branch.

In this example, Only actively modify the repository 
cluster-type-redis-repo, maybe with branches main (production), staging, and
however many dev branches are needed.
branches to represent cluster types.

I have not adapted the cardinality of trying to do this with multiple
accounts.  I'm open to contributions adding that without make a default
one-account system more complex.

So:
1 system repo: aws-salt
1 template repo: cluster-type-template
for each distinct cluster type:
1 cluster-<type>-default repo forked from cluster-type-template with definition of saltstate and saltclass
n branches, one for each distinct cluster in the default region

For each m <region> other than the default region deploying cluster type <type>:
m forks of cluster-<type>-default: cluster-<type>-<region>
(it is intended for the region definition to change in these forks, along with
perhaps the numbers of host classes in the various clusters)

Repo cluster-type-template has (in root of repo)
cluster-type.json
{ "cluster_type": <cluster-type-name>,  (e.g., "redis") (override when forking template)
  "repository_source": <whatever.git>, (e.g., "git@github.com:lago-morph/cluster-type-template.git")
  "region" : <region>, (e.g., us-east-1) (overwrite when forking cluster-<type>-default)
  "private_key_file" : <filename> (e.g., ~/secrets/default-ro-key)
}
Make sure repository_source matches the name of the new repository or you will
be sad (this is not checked!)
Don't store private_key_file in repository directory structure!

cluster.json (potentially different for each branch)
{ 
  "hostclass": 
    [
       {
         "class_name": <saltclass1>,
	     "num_hosts": <num>,
	     "public_ip": <true/false>
       },
       {
         "class_name": <saltclass2>,
	     ...
       },
       ...
    ]
}

===============================================================================
2024-11-29-2 Process design

0. Ensure the base images have been created
(This step only needs to be done once per account/region combination)

Clone the repository aws-simple.  
Configure the AWS CLI.  
Change directory to aws-simple/packer.
Run `packer init .` followed by `packer build .`
  (This takes 10-15 minutes typically, and can run in the background while
  doing the rest of this stuff)
Sometimes one or another of the builds fail.  You can do just one with one of
packer build -only=minion-build.amazon-ebs.ubuntu-20-04-amd64 .
packer build -only=master-build.amazon-ebs.ubuntu-20-04-amd64 .

1. Create the cluster-type repository.

Fork your new cluster-type repository from cluster-type-template.  Name the
new repository "cluster-<type>-default", where <type> is a descriptive name
(referred to as cluster_type_name below) like "redis" or something to describe 
this type of cluster.

Add the public key for a ssh keypair as a deploy-key in cluster-<type>-default.

Clone the new repo to your local filesystem.

In cluster-type.json, make sure you fill out the cluster-type-name, region,
and provide a filename on the local filesystem which contains the private key
for the deploy key.  In the future this may be replaced with something using
Vault, this is a workaround for now.

2. Upload private key to AWS.

Using scripts/aws_ssm_key.sh, copy the private deploy key to the 
System Manager Parameter Store on your AWS account.

3. Create a cluster using the cluster.json.  
If desired, change cluster.json to meet needs.  If desired, create a branch.

go to the terraform directory, and in succession type
make init (only required when the first time)
make plan (if you want to see what it will do)
make apply or make apply-auto-approve

If done with the Makefile, it will validate the region you are configured to
use, the region specified in cluster-type.json, and the region from the state
file (for an already-created cluster) are consistent.

When a cluster is created it uses the cluster_type_name and the branch to
create a cluster instance.  Cluster instances can be ephemeral, or could
correspond to long-lived functions like prod or staging.  aws-salt does not
use the Salt environment feature, and I try not to use that term to avoid
confusion.

Be careful about merging
branches that each have a cluster associated with them - the terraform state
for the cluster is stored in Git, and you don't want to accidentally overwrite
an active cluster state file with a merge.

4. Access your cluster
If you defined an ssh-bastion, then you can (from the terraform directory) do
make ssh-cluster

It may take a while for the cluster to build.  

===============================================================================
2024-11-29-3 making changes outlined above

Have to change a lot of the prototyping I did before to correspond to the new
data structure layout.

Lots and lots of just debugging-type changes for the terraform module.



Note, with packer found out (I think) why builds don't always work.  It turns
out that provisioning scripts start to run before cloud-init is done.
Cloud-init updates apt sources.  So if you start trying to install packages
before it is done, it sometimes fails.

Added a script that all it does is "cloud-init status --wait"
And that seemed (?) to fix the problem.

Ok, lots and lots of debugging, got the module working with the
cluster-type-template repository.

Note that I set the private key in SSM using a script in that repository
(scripts/set-private-key.sh).  I am not passing the private key through
Terraform at all.

===============================================================================
2024-11-29-4 prototype UserData script for salt-master

Now that I've changed all the naming conventions, I need to do the thing where
I manually set up the repository as a template for the UserData script on the
salt-master.

## set up the aws region
aws configure set region `curl --silent http://169.254.169.254/latest/dynamic/instance-identity/document | jq -r .region`

## grab the values we need from tags and SSM parameter
export CLUSTER_TYPE=$(curl http://169.254.169.254/latest/meta-data/tags/instance/cluster_type)
export REPOSITORY_SOURCE=$(curl http://169.254.169.254/latest/meta-data/tags/instance/repository_source)
export CLUSTER_NAME=$(curl http://169.254.169.254/latest/meta-data/tags/instance/cluster_name)
export SSM_SECRET_PATH=$(curl http://169.254.169.254/latest/meta-data/tags/instance/ssm_secret_path)
export BRANCH=$(curl http://169.254.169.254/latest/meta-data/tags/instance/branch)
export PRIVATE_KEY=$(aws ssm get-parameter --with-decryption --name $SSM_SECRET_PATH | jq -r .Parameter.Value)

## put back newlines where needed and set up private key for repository
echo $PRIVATE_KEY | sed "s/- /-\n/" | sed "s/ -/\n-/" > .ssh/repokey
chmod 600 .ssh/repokey
ssh-agent
ssh-add .ssh/repokey

export REPOSITORY=gitrepo 
GIT_SSH_COMMAND="ssh -o StrictHostKeyChecking=accept-new" git clone $REPOSITORY_SOURCE $REPOSITORY

## Then these same things from the other project git install script
sudo mv $REPOSITORY /opt
cd /opt/$REPOSITORY
git checkout $BRANCH
sudo ln -s /opt/$REPOSITORY/salt /srv/salt 
sudo ln -s /opt/$REPOSITORY/pillar /srv/pillar 
sudo ln -s /opt/$REPOSITORY/salt/master.d/* /etc/salt/master.d/

sudo systemctl restart salt-master

===============================================================================
2024-11-29-5 todos for next time

- Create UserData script for salt-master
- Configure private DNS server to point "salt" to salt-master
- Modify EC2 security group to only allow hosts to contact salt ports on master
- Basic salt config for webserver
- Script to populate saltclass hosts from instance tags
- cron job to periodically get instance hosts, apply state
- Make admin users parameter actually work in module
- Figure out how to taint hosts in Makefile in template terraform dir
- Start writing up documentation in README.md files in template

===============================================================================
2024-11-30 user data and DNS

Tried adding a user data script to salt-master.  It wasn't applied.  I read
that it will fail silently if there are extra spaces in the file.  So remove
the newlines I had for clarity and try again.

I can look at the user data on the instance with 
INSTANCE_ID=$(curl -s http://169.254.169.254/latest/meta-data/instance-id)
sudo cat /var/lib/cloud/instances/$INSTANCE_ID/user-data.txt

(from https://stackoverflow.com/questions/15904095/how-to-check-whether-my-user-data-passing-to-ec2-instance-is-working)

Turns out I was doing it on the hosts, not salt-master.  Ugh.

Now more debugging.  Everything is permission denied.  I'm not sure what user
the user data runs as.  Not sure if ubuntu user already exists.

Turns out I'd added a deploy key for testing the user functionality, rather
than my "default ro-key".  So added the public key for the private key I've
been using as a deploy key.

Turns out there is a world of difference between 
GIT_SSH_COMMAND="sudo ssh ..." sudo git clone ...
and 
GIT_SSH_COMMAND="ssh ..." git clone ...

And some of the other combinations.  For some reason sometimes when you do
"sudo git ..." it doesn't auth, but with just "git ..." it does, even when
running as root.  This was very strange.  But now it is fixed.

Added DNS for the hosts.  Created a private route 53 zone and added entries
for each of the hosts and the salt-master with their private IP addresses.

For some reason salt-master can't look up its own DNS record.  I'm confused.

Also, the file /etc/salt/minion_id is set during the packer run, and needs to
be removed as part of packer or explicitly configured in the UserData of the
hosts/salt-master.

Ok, the DNS records are correctly set, the hostnames are set, the identities
of the salt-minion key requests correspond to the Name keys for the instances.

So I guess we are ready now to build the hosts in saltclass with queries to
aws to find EC2 instances with the same cluster_name as the salt-master.

===============================================================================
2024-11-30-2 populating saltclass hosts.

Stupid jq tricks.

This gives me all the ec2 instances that are running in cluster template-main.
aws ec2 describe-instances --filter "Name=tag:cluster_name,Values=template-main"


The first part of this grabs all the instances in all the reservations.
The second part of this only passes through the instances that are running.
jq '.Reservations[].Instances[] | select(.State.Name == "running")'

We can restrict this down to just the IP addresses and the tags like this:
jq '{PrivateIpAddress, PublicIpAddress, Tags}'

(jq {foo, bar} is a shortcut for jq {foo: .foo, bar: .bar})

Having difficulty figuring out how to extract the values from the tags and
rearrange them.  For instance, I want to index by the Name tag, and have other
ones available at the same level as the IP addresses.

So I want:

{ "PrivateIpAddress": "10.0.64.43", 
  "PublicIpAddress": "18.212.135.48", 
  "Tags": [ 
      { "Key": "host_class",
        "Value": "salt-master"
	  },
      { "Key": "Name", 
		"Value": "salt-master" 
      }
  ]
}
{ "PrivateIpAddress": "10.0.73.92",
  "PublicIpAddress": "54.210.68.118",
  "Tags": [
	   { "Key": "host_class",
	     "Value": "public-webserver"
	   },
	   { "Key": "Name",
	     "Value": "public-webserver-0"
	   }
  ]
}

To turn into

{
      "PrivateIpAddress": "10.0.64.43", 
      "PublicIpAddress": "18.212.135.48", 
	  "hostname": "salt-master",
	  "host_class": "salt-master"
}
{ 
      "PrivateIpAddress": "10.0.73.92",
      "PublicIpAddress": "54.210.68.118",
	  "hostname": "public-webserver-0"
      "host_class": "public-webserver",
}

And, I finally got it.

aws ec2 describe-instances --filter "Name=tag:cluster_name,Values=template-main" \
	| jq '.Reservations[].Instances[] | select(.State.Name == "running")' \
	| jq '{private_ip: .PrivateIpAddress, 
	       public_ip: .PublicIpAddress, 
		   host_name: (.Tags[] | select(.Key == "Name").Value), 
		   host_class: (.Tags[] | select(.Key == "host_class").Value)}'

I think there must be a better way to "reduce" the tags to first-class
attributes.  I just don't know jq well enough.

This is the output of that command with my current cluster:

{
  "private_ip": "10.0.72.116",
  "public_ip": "50.17.77.66",
  "host_name": "bastion-0",
  "host_class": "bastion"
}
{
  "private_ip": "10.0.73.92",
  "public_ip": "54.210.68.118",
  "host_name": "public-webserver-0",
  "host_class": "public-webserver"
}
{
  "private_ip": "10.0.64.43",
  "public_ip": "18.212.135.48",
  "host_name": "salt-master",
  "host_class": "salt-master"
}

Anyway, getting late, will finish up converting this into saltclass host files
tomorrow.

===============================================================================
2024-12-01 setting up saltclass nodes

I added permissions to the salt-master instance role to query EC2 (read-only).

I want the folowing record:
{
  "private_ip": "10.0.64.43",
  "public_ip": "18.212.135.48",
  "host_name": "salt-master",
  "host_class": "salt-master"
}

To turn into the following file:

<saltclass_path>/nodes/salt-master.yml
environment: base

classes:
  - salt-master

pillar:
  network:
    public_ip: "18.212.135.48"
    private_ip: "10.0.64.43"


I did a ton of fiddling to change around my jq manipulation.  This is what I
have so far, but I need to take a break so I'm just recording my progress.

#!/bin/bash

CLUSTER_NAME=$(curl http://169.254.169.254/latest/meta-data/tags/instance/cluster_name)

all_instances=$(aws ec2 describe-instances --filter "Name=tag:cluster_name,Values=$CLUSTER_NAME")

running_instances=$(jq -r '[.Reservations[].Instances[] | select(.State.Name == "running")]' <<< $all_instances)

# puts the data in a Key/Value pair with the Key being the "Name" tag and the
# Value being an object with private_ip, public_ip, and host_class.
nkey=$(jq -r '[.[] |
              (.Tags | from_entries) as $tags | 
              {Key: $tags.Name, Value: ({private_ip: .PrivateIpAddress, public_ip: .PublicIpAddress, host_class: $tags.host_class})}]' <<< $running_instances)

# output the data as pairs of lines, host_name, and compact 1-line
# representation of the object with three data lines.
# I then process two lines at a time, passing the json object into jinja2

template=/tmp/node.jinja
destination=/tmp
jq -r -c '.[] | .Key, .Value' <<< $nkey | 
while read -r host_name; read -r host_data; do 
    echo "hostname: $host_name"; 
	(echo $host_data | jinja2 $template) > $destination/$host_name.yml; 
done

--- template node.jinja
environment: base

classes:
  - {{ host_class }}

pillar:
  network:
    public_ip: {{ public_ip }}
    private_ip: {{ private_ip }}
---

Need to install some packages on salt-master for this
sudo apt-get install pipx python3-venv -y
pipx install jinja2-cli
pipx ensurepath
. .bashrc

Added this to the install-extras.sh script that is run as part of the packer
build.  Root will have jinja2 on its path, but maybe from the script call
jinja2 including the path just to be sure.  

E.g.,
/root/.local/bin/jinja2


